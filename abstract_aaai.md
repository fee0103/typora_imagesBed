## [1] Segment Any 3D Gaussians

### Segment Any 3D Gaussians

Interactive 3D segmentation in radiance fields is an appealing task since its importance in 3D scene understanding and manipulation. However, existing methods face challenges in either achieving fine-grained, multi-granularity segmentation or contending with substantial computational overhead, inhibiting real-time interaction. In this paper, we introduce Segment Any 3D GAussians (SAGA), a novel 3D interactive segmentation approach that seamlessly blends a 2D segmentation foundation model with 3D Gaussian Splatting (3DGS), a recent breakthrough of radiance fields. SAGA efficiently embeds multi-granularity 2D segmentation results generated by the segmentation foundation model into 3D Gaussian point features through well-designed contrastive training. Evaluation on existing benchmarks demonstrates that SAGA can achieve competitive performance with state-of-the-art methods. Moreover, SAGA achieves multi-granularity segmentation and accommodates various prompts, including points, scribbles, and 2D masks. Notably, SAGA can finish the 3D segmentation within milliseconds, achieving nearly 1000x acceleration compared to previous SOTA.

交互式3D辐射场分割是一个吸引人的任务，因为它在3D场景理解和操纵中非常重要。然而，现有方法在实现细粒度、多粒度分割或应对大量计算开销方面面临挑战，这限制了实时互动。在这篇论文中，我们介绍了“分割任意3D高斯”（SAGA），这是一种新颖的3D交互式分割方法，它将2D分割基础模型与3D高斯喷溅（3DGS）无缝融合，后者是辐射场的最新突破。SAGA通过精心设计的对比训练，高效地将分割基础模型生成的多粒度2D分割结果嵌入到3D高斯点特征中。在现有基准测试上的评估表明，SAGA能够与最先进的方法竞争。此外，SAGA实现了多粒度分割，并适应各种提示，包括点、涂鸦和2D蒙版。值得注意的是，SAGA可以在几毫秒内完成3D分割，与以前的最先进技术相比，几乎实现了1000倍的加速。


---

## [2] GFlow: Recovering 4D World from Monocular Video

### GFlow: Recovering 4D World from Monocular Video

Reconstructing 4D scenes from video inputs is a crucial yet challenging task. Conventional methods usually rely on the assumptions of multi-view video inputs, known camera parameters, or static scenes, all of which are typically absent under in-the-wild scenarios. In this paper, we relax all these constraints and tackle a highly ambitious but practical task, which we termed as AnyV4D: we assume only one monocular video is available without any camera parameters as input, and we aim to recover the dynamic 4D world alongside the camera poses. To this end, we introduce GFlow, a new framework that utilizes only 2D priors (depth and optical flow) to lift a video (3D) to a 4D explicit representation, entailing a flow of Gaussian splatting through space and time. GFlow first clusters the scene into still and moving parts, then applies a sequential optimization process that optimizes camera poses and the dynamics of 3D Gaussian points based on 2D priors and scene clustering, ensuring fidelity among neighboring points and smooth movement across frames. Since dynamic scenes always introduce new content, we also propose a new pixel-wise densification strategy for Gaussian points to integrate new visual content. Moreover, GFlow transcends the boundaries of mere 4D reconstruction; it also enables tracking of any points across frames without the need for prior training and segments moving objects from the scene in an unsupervised way. Additionally, the camera poses of each frame can be derived from GFlow, allowing for rendering novel views of a video scene through changing camera pose. By employing the explicit representation, we may readily conduct scene-level or object-level editing as desired, underscoring its versatility and power.

从视频输入中重建4D场景是一项至关重要但又极具挑战性的任务。传统方法通常依赖于多视角视频输入、已知的相机参数或静态场景的假设，这些在野外环境中通常是不存在的。在这篇论文中，我们放宽了所有这些限制，并解决了一个非常雄心勃勃但实际的任务，我们称之为 AnyV4D：我们假设只有一个单目视频可用，没有任何相机参数作为输入，我们的目标是恢复动态的4D世界以及相机姿态。为此，我们引入了 GFlow，这是一个新框架，仅使用2D先验（深度和光流）将视频（3D）提升到一个4D显式表征，包括通过时空的高斯喷溅流动。GFlow首先将场景聚类为静止部分和移动部分，然后应用一个序列优化过程，该过程基于2D先验和场景聚类优化相机姿态和3D高斯点的动态，确保相邻点之间的保真度和帧间的平滑运动。由于动态场景总是引入新内容，我们还提出了一种新的高斯点的像素级密集化策略，以整合新的视觉内容。此外，GFlow超越了单纯的4D重建的界限；它还可以在无需事先训练的情况下跟踪任何帧之间的点，并以无监督的方式从场景中分割移动对象。此外，可以从GFlow派生每个帧的相机姿态，允许通过改变相机姿态渲染视频场景的新视角。通过采用显式表征，我们可以根据需要进行场景级或对象级编辑，突显其多功能性和强大性。


---

## [3] DreamPhysics: Learning Physical Properties of Dynamic 3D Gaussians with Video Diffusion Priors

### DreamPhysics: Learning Physical Properties of Dynamic 3D Gaussians with Video Diffusion Priors\

Dynamic 3D interaction has witnessed great interest in recent works, while creating such 4D content remains challenging. One solution is to animate 3D scenes with physics-based simulation, and the other is to learn the deformation of static 3D objects with the distillation of video generative models. The former one requires assigning precise physical properties to the target object, otherwise the simulated results would become unnatural. The latter tends to formulate the video with minor motions and discontinuous frames, due to the absence of physical constraints in deformation learning. We think that video generative models are trained with real-world captured data, capable of judging physical phenomenon in simulation environments. To this end, we propose DreamPhysics in this work, which estimates physical properties of 3D Gaussian Splatting with video diffusion priors. DreamPhysics supports both image- and text-conditioned guidance, optimizing physical parameters via score distillation sampling with frame interpolation and log gradient. Based on a material point method simulator with proper physical parameters, our method can generate 4D content with realistic motions. Experimental results demonstrate that, by distilling the prior knowledge of video diffusion models, inaccurate physical properties can be gradually refined for high-quality simulation.

动态 3D 交互在近期的研究中引起了极大的兴趣，而创建此类 4D 内容仍然具有挑战性。一种解决方案是通过物理基础的模拟来动画化 3D 场景，另一种则是通过提炼视频生成模型来学习静态 3D 对象的形变。前者需要为目标对象分配精确的物理属性，否则模拟结果可能会变得不自然。后者由于在形变学习中缺乏物理约束，往往会导致视频中的微小运动和不连续的帧。我们认为视频生成模型是通过现实世界捕获的数据训练而成的，能够在模拟环境中判断物理现象。为此，我们在这项工作中提出了 DreamPhysics，它利用视频扩散先验估计 3D 高斯喷溅的物理属性。DreamPhysics 支持图像和文本条件指导，通过得分提炼采样与帧插值和对数梯度来优化物理参数。基于具有适当物理参数的材料点方法模拟器，我们的方法可以生成具有真实运动的 4D 内容。实验结果表明，通过提炼视频扩散模型的先验知识，可以逐渐完善不准确的物理属性，以实现高质量的模拟。


---

## [4] FastLGS: Speeding up Language Embedded Gaussians with Feature Grid Mapping

### FastLGS: Speeding up Language Embedded Gaussians with Feature Grid Mapping

The semantically interactive radiance field has always been an appealing task for its potential to facilitate user-friendly and automated real-world 3D scene understanding applications. However, it is a challenging task to achieve high quality, efficiency and zero-shot ability at the same time with semantics in radiance fields. In this work, we present FastLGS, an approach that supports real-time open-vocabulary query within 3D Gaussian Splatting (3DGS) under high resolution. We propose the semantic feature grid to save multi-view CLIP features which are extracted based on Segment Anything Model (SAM) masks, and map the grids to low dimensional features for semantic field training through 3DGS. Once trained, we can restore pixel-aligned CLIP embeddings through feature grids from rendered features for open-vocabulary queries. Comparisons with other state-of-the-art methods prove that FastLGS can achieve the first place performance concerning both speed and accuracy, where FastLGS is 98x faster than LERF and 4x faster than LangSplat. Meanwhile, experiments show that FastLGS is adaptive and compatible with many downstream tasks, such as 3D segmentation and 3D object inpainting, which can be easily applied to other 3D manipulation systems.

语义交互式辐射场一直是一个吸引人的任务，因为它有助于促进用户友好和自动化的现实世界3D场景理解应用。然而，在辐射场中同时实现高质量、高效率和零样本能力是一个挑战性任务。在这项工作中，我们提出了FastLGS，这是一种支持实时开放词汇查询的方法，适用于高分辨率下的3D高斯涂抹（3DGS）。我们提出了语义特征网格来保存基于Segment Anything Model（SAM）掩码提取的多视图CLIP特征，并通过3DGS将网格映射到低维特征进行语义场训练。一旦训练完成，我们可以通过从渲染特征中恢复的特征网格恢复像素对齐的CLIP嵌入，以进行开放词汇查询。与其他最先进方法的比较证明，FastLGS在速度和准确性方面均能取得第一名的表现，其中FastLGS比LERF快98倍，比LangSplat快4倍。同时，实验表明FastLGS能够适应并兼容许多下游任务，例如3D分割和3D对象修复，这些任务可以轻松应用于其他3D操作系统。


---

## [5] TranSplat: Generalizable 3D Gaussian Splatting from Sparse Multi-View Images with Transformers

### TranSplat: Generalizable 3D Gaussian Splatting from Sparse Multi-View Images with Transformers

Compared with previous 3D reconstruction methods like Nerf, recent Generalizable 3D Gaussian Splatting (G-3DGS) methods demonstrate impressive efficiency even in the sparse-view setting. However, the promising reconstruction performance of existing G-3DGS methods relies heavily on accurate multi-view feature matching, which is quite challenging. Especially for the scenes that have many non-overlapping areas between various views and contain numerous similar regions, the matching performance of existing methods is poor and the reconstruction precision is limited. To address this problem, we develop a strategy that utilizes a predicted depth confidence map to guide accurate local feature matching. In addition, we propose to utilize the knowledge of existing monocular depth estimation models as prior to boost the depth estimation precision in non-overlapping areas between views. Combining the proposed strategies, we present a novel G-3DGS method named TranSplat, which obtains the best performance on both the RealEstate10K and ACID benchmarks while maintaining competitive speed and presenting strong cross-dataset generalization ability.

与之前的 3D 重建方法如 Nerf 相比，最近的通用 3D 高斯点云（G-3DGS）方法在稀疏视图设置下表现出令人印象深刻的效率。然而，现有 G-3DGS 方法的良好重建性能在很大程度上依赖于准确的多视图特征匹配，这非常具有挑战性。特别是对于那些各视图之间有许多非重叠区域并且包含大量相似区域的场景，现有方法的匹配性能较差，重建精度有限。为了解决这个问题，我们开发了一种利用预测深度置信度图来引导准确局部特征匹配的策略。此外，我们建议利用现有单目深度估计模型的知识作为先验，以提升视图间非重叠区域的深度估计精度。结合这些策略，我们提出了一种新型 G-3DGS 方法，命名为 TranSplat，它在 RealEstate10K 和 ACID 基准测试中均获得了最佳性能，同时保持了竞争的速度，并展示了强大的跨数据集泛化能力。



---

## [6] DrivingForward: Feed-forward 3D Gaussian Splatting for Driving Scene Reconstruction from Flexible Surround-view Input

### DrivingForward: Feed-forward 3D Gaussian Splatting for Driving Scene Reconstruction from Flexible Surround-view Input

We propose DrivingForward, a feed-forward Gaussian Splatting model that reconstructs driving scenes from flexible surround-view input. Driving scene images from vehicle-mounted cameras are typically sparse, with limited overlap, and the movement of the vehicle further complicates the acquisition of camera extrinsics. To tackle these challenges and achieve real-time reconstruction, we jointly train a pose network, a depth network, and a Gaussian network to predict the Gaussian primitives that represent the driving scenes. The pose network and depth network determine the position of the Gaussian primitives in a self-supervised manner, without using depth ground truth and camera extrinsics during training. The Gaussian network independently predicts primitive parameters from each input image, including covariance, opacity, and spherical harmonics coefficients. At the inference stage, our model can achieve feed-forward reconstruction from flexible multi-frame surround-view input. Experiments on the nuScenes dataset show that our model outperforms existing state-of-the-art feed-forward and scene-optimized reconstruction methods in terms of reconstruction.

我们提出了DrivingForward，一种基于前馈高斯散点模型的驾驶场景重建方法，能够从灵活的环视输入中进行重建。来自车辆安装摄像头的驾驶场景图像通常较为稀疏，且重叠区域有限，同时车辆的移动进一步加大了摄像机外参获取的难度。为了解决这些挑战并实现实时重建，我们联合训练了姿态网络、深度网络和高斯网络，以预测代表驾驶场景的高斯基元。姿态网络和深度网络以自监督的方式确定高斯基元的位置，在训练过程中无需深度真值和摄像机外参。高斯网络则独立预测每个输入图像的基元参数，包括协方差、透明度和球谐系数。在推理阶段，我们的模型能够从灵活的多帧环视输入中实现前馈重建。基于nuScenes数据集的实验表明，我们的模型在重建性能上优于现有的前馈和场景优化重建方法。


---

## [7] Bootstraping Clustering of Gaussians for View-consistent 3D Scene Understanding

### Bootstraping Clustering of Gaussians for View-consistent 3D Scene Understanding

Injecting semantics into 3D Gaussian Splatting (3DGS) has recently garnered significant attention. While current approaches typically distill 3D semantic features from 2D foundational models (e.g., CLIP and SAM) to facilitate novel view segmentation and semantic understanding, their heavy reliance on 2D supervision can undermine cross-view semantic consistency and necessitate complex data preparation processes, therefore hindering view-consistent scene understanding. In this work, we present FreeGS, an unsupervised semantic-embedded 3DGS framework that achieves view-consistent 3D scene understanding without the need for 2D labels. Instead of directly learning semantic features, we introduce the IDentity-coupled Semantic Field (IDSF) into 3DGS, which captures both semantic representations and view-consistent instance indices for each Gaussian. We optimize IDSF with a two-step alternating strategy: semantics help to extract coherent instances in 3D space, while the resulting instances regularize the injection of stable semantics from 2D space. Additionally, we adopt a 2D-3D joint contrastive loss to enhance the complementarity between view-consistent 3D geometry and rich semantics during the bootstrapping process, enabling FreeGS to uniformly perform tasks such as novel-view semantic segmentation, object selection, and 3D object detection. Extensive experiments on LERF-Mask, 3D-OVS, and ScanNet datasets demonstrate that FreeGS performs comparably to state-of-the-art methods while avoiding the complex data preprocessing workload.

在 3D 高斯点云表示（3D Gaussian Splatting, 3DGS）中注入语义信息，近年来引起了广泛关注。尽管当前的方法通常依赖从 2D 基础模型（如 CLIP 和 SAM）提取 3D 语义特征，以促进新视图分割和语义理解，但其对 2D 监督的高度依赖可能会削弱跨视图语义一致性，同时需要复杂的数据准备过程，从而阻碍了视图一致的场景理解。
为此，我们提出了 FreeGS，一种无监督的语义嵌入 3DGS 框架，可在无需 2D 标签的情况下实现视图一致的 3D 场景理解。与直接学习语义特征不同，我们引入了 身份耦合语义场（IDentity-coupled Semantic Field, IDSF） 到 3DGS 中，该方法为每个高斯点捕获语义表示和视图一致的实例索引。
我们通过两步交替策略优化 IDSF：语义用于在 3D 空间中提取一致的实例，而提取出的实例则对从 2D 空间注入稳定语义起到正则化作用。此外，我们采用 2D-3D 联合对比损失，增强视图一致的 3D 几何与丰富语义之间的互补性，在引导过程中支持 FreeGS 统一执行多种任务，例如新视图语义分割、对象选择和 3D 对象检测。
在 LERF-Mask、3D-OVS 和 ScanNet 数据集上的广泛实验表明，FreeGS 的性能与当前最先进的方法相当，同时避免了复杂的数据预处理工作量。这验证了 FreeGS 的高效性和实用性，为语义注入的 3D 场景理解提供了一种新方向。


---

## [8] PointTalk: Audio-Driven Dynamic Lip Point Cloud for 3D Gaussian-based Talking Head Synthesis

### PointTalk: Audio-Driven Dynamic Lip Point Cloud for 3D Gaussian-based Talking Head Synthesis

Talking head synthesis with arbitrary speech audio is a crucial challenge in the field of digital humans. Recently, methods based on radiance fields have received increasing attention due to their ability to synthesize high-fidelity and identity-consistent talking heads from just a few minutes of training video. However, due to the limited scale of the training data, these methods often exhibit poor performance in audio-lip synchronization and visual quality. In this paper, we propose a novel 3D Gaussian-based method called PointTalk, which constructs a static 3D Gaussian field of the head and deforms it in sync with the audio. It also incorporates an audio-driven dynamic lip point cloud as a critical component of the conditional information, thereby facilitating the effective synthesis of talking heads. Specifically, the initial step involves generating the corresponding lip point cloud from the audio signal and capturing its topological structure. The design of the dynamic difference encoder aims to capture the subtle nuances inherent in dynamic lip movements more effectively. Furthermore, we integrate the audio-point enhancement module, which not only ensures the synchronization of the audio signal with the corresponding lip point cloud within the feature space, but also facilitates a deeper understanding of the interrelations among cross-modal conditional features. Extensive experiments demonstrate that our method achieves superior high-fidelity and audio-lip synchronization in talking head synthesis compared to previous methods.

使用任意语音音频生成数字人头部的说话动画是数字人领域的关键挑战。近年来，基于辐射场的方法因其能够从仅几分钟的训练视频中生成高保真且身份一致的说话头像而受到越来越多的关注。然而，由于训练数据规模有限，这些方法通常在音频与唇形的同步性及视觉质量方面表现欠佳。
为了解决这些问题，我们提出了一种新颖的基于3D高斯的生成方法，称为 PointTalk。该方法通过构建一个静态的头部3D高斯场，并根据音频进行同步变形。同时，PointTalk 引入了一个由音频驱动的动态唇部点云作为条件信息的关键组成部分，从而有效促进了说话头像的合成。
具体而言，PointTalk 的初始步骤是从音频信号生成对应的唇部点云并捕捉其拓扑结构。设计的动态差分编码器（Dynamic Difference Encoder）旨在更有效地捕捉唇部动态运动中细微的变化。此外，我们集成了音频-点云增强模块（Audio-Point Enhancement Module），不仅在特征空间中确保音频信号与对应唇部点云的同步性，还促进了跨模态条件特征之间关系的深入理解。
大量实验表明，与现有方法相比，我们的方法在说话头像合成的高保真度和音频-唇形同步性方面均表现出色，为该领域的进一步发展提供了新的技术基础。


---

## [9] 3D2-Actor: Learning Pose-Conditioned 3D-Aware Denoiser for Realistic Gaussian Avatar Modeling

### 3D2-Actor: Learning Pose-Conditioned 3D-Aware Denoiser for Realistic Gaussian Avatar Modeling

Advancements in neural implicit representations and differentiable rendering have markedly improved the ability to learn animatable 3D avatars from sparse multi-view RGB videos. However, current methods that map observation space to canonical space often face challenges in capturing pose-dependent details and generalizing to novel poses. While diffusion models have demonstrated remarkable zero-shot capabilities in 2D image generation, their potential for creating animatable 3D avatars from 2D inputs remains underexplored. In this work, we introduce 3D2-Actor, a novel approach featuring a pose-conditioned 3D-aware human modeling pipeline that integrates iterative 2D denoising and 3D rectifying steps. The 2D denoiser, guided by pose cues, generates detailed multi-view images that provide the rich feature set necessary for high-fidelity 3D reconstruction and pose rendering. Complementing this, our Gaussian-based 3D rectifier renders images with enhanced 3D consistency through a two-stage projection strategy and a novel local coordinate representation. Additionally, we propose an innovative sampling strategy to ensure smooth temporal continuity across frames in video synthesis. Our method effectively addresses the limitations of traditional numerical solutions in handling ill-posed mappings, producing realistic and animatable 3D human avatars. Experimental results demonstrate that 3D2-Actor excels in high-fidelity avatar modeling and robustly generalizes to novel poses.

神经隐式表示和可微渲染的进步显著提升了从稀疏多视角RGB视频中学习可动画3D角色的能力。然而，目前将观察空间映射到规范空间的方法在捕捉依赖姿态的细节以及推广到新姿态时常面临挑战。尽管扩散模型在2D图像生成中展现了卓越的零样本能力，但其在从2D输入中创建可动画3D角色的潜力仍未被充分挖掘。
在这项工作中，我们提出了 3D2-Actor，一种新颖的姿态条件3D感知人类建模管线，集成了迭代的2D去噪和3D校正步骤。2D去噪模块通过姿态线索引导，生成详细的多视角图像，为高保真3D重建和姿态渲染提供了丰富的特征集合。与此相辅相成，我们的基于高斯的3D校正模块通过两阶段投影策略和一种新颖的局部坐标表示，渲染出具有增强3D一致性的图像。此外，我们提出了一种创新的采样策略，确保视频合成中跨帧的平滑时间连续性。
我们的方法有效解决了传统数值解法在处理病态映射时的局限性，生成真实且可动画的3D人类角色。实验结果表明，3D2-Actor 在高保真角色建模方面表现卓越，并能够稳健地推广到新姿态。


---

## [10] GraphAvatar: Compact Head Avatars with GNN-Generated 3D Gaussians

### GraphAvatar: Compact Head Avatars with GNN-Generated 3D Gaussians

Rendering photorealistic head avatars from arbitrary viewpoints is crucial for various applications like virtual reality. Although previous methods based on Neural Radiance Fields (NeRF) can achieve impressive results, they lack fidelity and efficiency. Recent methods using 3D Gaussian Splatting (3DGS) have improved rendering quality and real-time performance but still require significant storage overhead. In this paper, we introduce a method called GraphAvatar that utilizes Graph Neural Networks (GNN) to generate 3D Gaussians for the head avatar. Specifically, GraphAvatar trains a geometric GNN and an appearance GNN to generate the attributes of the 3D Gaussians from the tracked mesh. Therefore, our method can store the GNN models instead of the 3D Gaussians, significantly reducing the storage overhead to just 10MB. To reduce the impact of face-tracking errors, we also present a novel graph-guided optimization module to refine face-tracking parameters during training. Finally, we introduce a 3D-aware enhancer for post-processing to enhance the rendering quality. We conduct comprehensive experiments to demonstrate the advantages of GraphAvatar, surpassing existing methods in visual fidelity and storage consumption. The ablation study sheds light on the trade-offs between rendering quality and model size.

从任意视角渲染光真实感的头像是虚拟现实等多种应用中的关键任务。尽管基于神经辐射场（NeRF）的现有方法能够取得令人印象深刻的效果，但在保真度和效率方面仍存在不足。近期基于三维高斯点云（3D Gaussian Splatting, 3DGS）的方法改善了渲染质量并实现了实时性能，但其存储开销仍然较高。
为了解决这一问题，我们提出了 GraphAvatar，一种利用图神经网络（Graph Neural Networks, GNN）生成头像三维高斯的方法。具体而言，GraphAvatar 通过训练几何 GNN 和外观 GNN，从追踪的网格中生成三维高斯的属性。因此，我们的方法仅需存储 GNN 模型，而不需要存储三维高斯本身，将存储开销显著降低至仅 10MB。
为减轻面部追踪误差的影响，我们引入了一种基于图引导的优化模块，用于在训练过程中优化面部追踪参数。此外，我们提出了一个三维感知增强模块，用于后处理以提升渲染质量。
通过全面实验，我们验证了 GraphAvatar 在视觉保真度和存储消耗方面的显著优势。消融研究进一步探讨了渲染质量与模型大小之间的权衡，表明 GraphAvatar 在性能与存储效率上的平衡具有重要意义。



---

## [11] EGSRAL: An Enhanced 3D Gaussian Splatting based Renderer with Automated Labeling for Large-Scale Driving Scene

### EGSRAL: An Enhanced 3D Gaussian Splatting based Renderer with Automated Labeling for Large-Scale Driving Scene

3D Gaussian Splatting (3D GS) has gained popularity due to its faster rendering speed and high-quality novel view synthesis. Some researchers have explored using 3D GS for reconstructing driving scenes. However, these methods often rely on various data types, such as depth maps, 3D boxes, and trajectories of moving objects. Additionally, the lack of annotations for synthesized images limits their direct application in downstream tasks. To address these issues, we propose EGSRAL, a 3D GS-based method that relies solely on training images without extra annotations. EGSRAL enhances 3D GS's capability to model both dynamic objects and static backgrounds and introduces a novel adaptor for auto labeling, generating corresponding annotations based on existing annotations. We also propose a grouping strategy for vanilla 3D GS to address perspective issues in rendering large-scale, complex scenes. Our method achieves state-of-the-art performance on multiple datasets without any extra annotation. For example, the PSNR metric reaches 29.04 on the nuScenes dataset. Moreover, our automated labeling can significantly improve the performance of 2D/3D detection tasks.

3D高斯点绘（3D Gaussian Splatting, 3D GS）因其快速渲染速度和高质量的新视角合成能力而受到广泛关注。一些研究者已探索将3D GS应用于驾驶场景重建。然而，这些方法通常依赖多种数据类型，例如深度图、3D框以及动态物体的轨迹。此外，合成图像缺乏标注，限制了其在下游任务中的直接应用。
为了解决这些问题，我们提出了一种基于3D GS的新方法 EGSRAL，该方法完全依赖训练图像而无需额外标注。EGSRAL增强了3D GS在建模动态物体和静态背景方面的能力，并引入了一种新颖的自动标注适配器，能够基于已有标注生成相应的注释。此外，我们提出了一种针对基础3D GS的分组策略，用以解决在渲染大规模复杂场景时的透视问题。
我们的方法在多个数据集上实现了最先进的性能，无需额外标注。例如，在 nuScenes 数据集上，PSNR 指标达到了29.04。此外，我们的自动化标注功能显著提高了2D/3D检测任务的性能。


---

## [12] Topology-Aware 3D Gaussian Splatting: Leveraging Persistent Homology for Optimized Structural Integrity

### Topology-Aware 3D Gaussian Splatting: Leveraging Persistent Homology for Optimized Structural Integrity

Gaussian Splatting (GS) has emerged as a crucial technique for representing discrete volumetric radiance fields. It leverages unique parametrization to mitigate computational demands in scene optimization. This work introduces Topology-Aware 3D Gaussian Splatting (Topology-GS), which addresses two key limitations in current approaches: compromised pixel-level structural integrity due to incomplete initial geometric coverage, and inadequate feature-level integrity from insufficient topological constraints during optimization. To overcome these limitations, Topology-GS incorporates a novel interpolation strategy, Local Persistent Voronoi Interpolation (LPVI), and a topology-focused regularization term based on persistent barcodes, named PersLoss. LPVI utilizes persistent homology to guide adaptive interpolation, enhancing point coverage in low-curvature areas while preserving topological structure. PersLoss aligns the visual perceptual similarity of rendered images with ground truth by constraining distances between their topological features. Comprehensive experiments on three novel-view synthesis benchmarks demonstrate that Topology-GS outperforms existing methods in terms of PSNR, SSIM, and LPIPS metrics, while maintaining efficient memory usage. This study pioneers the integration of topology with 3D-GS, laying the groundwork for future research in this area.

高斯点绘（Gaussian Splatting, GS）已成为表示离散体积辐射场的重要技术，它通过独特的参数化方法降低场景优化中的计算需求。本文提出了拓扑感知3D高斯点绘（Topology-GS），以解决当前方法的两个关键限制：由于初始几何覆盖不完全导致的像素级结构完整性受损，以及由于优化过程中缺乏足够的拓扑约束导致的特征级完整性不足。
为克服这些限制，Topology-GS 引入了一种新颖的插值策略 局部持久Voronoi插值（Local Persistent Voronoi Interpolation, LPVI），以及基于持久条形码的拓扑聚焦正则项 PersLoss。LPVI 利用持久同调（persistent homology）引导自适应插值，在低曲率区域增强点覆盖，同时保留拓扑结构。PersLoss 通过约束渲染图像与真实图像的拓扑特征之间的距离，将视觉感知相似性与真实场景对齐。
在三个新视角合成基准数据集上的全面实验表明，Topology-GS 在 PSNR、SSIM 和 LPIPS 指标上均优于现有方法，同时保持高效的内存使用。该研究开创性地将拓扑与3D-GS相结合，为该领域未来的研究奠定了基础。


---

## [13] GaussianPainter: Painting Point Cloud into 3D Gaussians with Normal Guidance

### GaussianPainter: Painting Point Cloud into 3D Gaussians with Normal Guidance

In this paper, we present GaussianPainter, the first method to paint a point cloud into 3D Gaussians given a reference image. GaussianPainter introduces an innovative feed-forward approach to overcome the limitations of time-consuming test-time optimization in 3D Gaussian splatting. Our method addresses a critical challenge in the field: the non-uniqueness problem inherent in the large parameter space of 3D Gaussian splatting. This space, encompassing rotation, anisotropic scales, and spherical harmonic coefficients, introduces the challenge of rendering similar images from substantially different Gaussian fields. As a result, feed-forward networks face instability when attempting to directly predict high-quality Gaussian fields, struggling to converge on consistent parameters for a given output. To address this issue, we propose to estimate a surface normal for each point to determine its Gaussian rotation. This strategy enables the network to effectively predict the remaining Gaussian parameters in the constrained space. We further enhance our approach with an appearance injection module, incorporating reference image appearance into Gaussian fields via a multiscale triplane representation. Our method successfully balances efficiency and fidelity in 3D Gaussian generation, achieving high-quality, diverse, and robust 3D content creation from point clouds in a single forward pass.

在本文中，我们提出了GaussianPainter，这是第一个在给定参考图像的情况下将点云绘制为三维高斯点云的方法。GaussianPainter引入了一种创新的前馈方法，克服了三维高斯点云中耗时的测试时优化的限制。我们的方法解决了该领域的一个关键挑战：三维高斯点云的大参数空间固有的非唯一性问题。这个参数空间包括旋转、各向异性缩放和球面谐系数，这导致从截然不同的高斯场渲染出相似图像成为一大挑战。因此，前馈网络在尝试直接预测高质量的高斯场时会面临不稳定性，难以在给定输出上收敛到一致的参数。为了解决这个问题，我们提出为每个点估计一个表面法线，以确定其高斯旋转。该策略使网络能够在受限的空间内有效地预测其余的高斯参数。我们进一步通过外观注入模块增强了我们的方法，通过多尺度三平面表示将参考图像的外观融入高斯场。我们的方法在三维高斯生成中成功地平衡了效率和保真度，实现了从点云中在单次前馈过程中生成高质量、多样化且鲁棒的三维内容。


---

## [14] KeyGS: A Keyframe-Centric Gaussian Splatting Method for Monocular Image Sequences

### KeyGS: A Keyframe-Centric Gaussian Splatting Method for Monocular Image Sequences

Reconstructing high-quality 3D models from sparse 2D images has garnered significant attention in computer vision. Recently, 3D Gaussian Splatting (3DGS) has gained prominence due to its explicit representation with efficient training speed and real-time rendering capabilities. However, existing methods still heavily depend on accurate camera poses for reconstruction. Although some recent approaches attempt to train 3DGS models without the Structure-from-Motion (SfM) preprocessing from monocular video datasets, these methods suffer from prolonged training times, making them impractical for many applications. In this paper, we present an efficient framework that operates without any depth or matching model. Our approach initially uses SfM to quickly obtain rough camera poses within seconds, and then refines these poses by leveraging the dense representation in 3DGS. This framework effectively addresses the issue of long training times. Additionally, we integrate the densification process with joint refinement and propose a coarse-to-fine frequency-aware densification to reconstruct different levels of details. This approach prevents camera pose estimation from being trapped in local minima or drifting due to high-frequency signals. Our method significantly reduces training time from hours to minutes while achieving more accurate novel view synthesis and camera pose estimation compared to previous methods.

从稀疏二维图像重建高质量三维模型是计算机视觉中的一个重要研究方向。近年来，三维高斯散射（3D Gaussian Splatting, 3DGS）因其显式表示、高效的训练速度和实时渲染能力而受到广泛关注。然而，现有方法在重建中仍然严重依赖于精确的相机位姿。尽管一些最新方法尝试在单目视频数据集上训练3DGS模型而无需依赖结构化运动（Structure-from-Motion, SfM）预处理，这些方法却由于训练时间过长而在许多应用场景中变得不切实际。
在本文中，我们提出了一个无需深度或匹配模型的高效框架。我们的方法首先通过SfM在几秒内快速获得粗略的相机位姿，然后利用3DGS的密集表示对这些位姿进行优化，从而有效解决了长时间训练的问题。此外，我们将密集化过程与联合优化相结合，提出了一种从粗到细的频率感知密集化方法，用于重建不同层次的细节。该方法能够避免相机位姿估计因高频信号陷入局部最小值或发生漂移。
与现有方法相比，我们的方法显著减少了训练时间，从数小时缩短至数分钟，同时在新视角合成和相机位姿估计的准确性上实现了更优的性能。


---

## [15] HOGSA: Bimanual Hand-Object Interaction Understanding with 3D Gaussian Splatting Based Data Augmentation

### HOGSA: Bimanual Hand-Object Interaction Understanding with 3D Gaussian Splatting Based Data Augmentation

Understanding of bimanual hand-object interaction plays an important role in robotics and virtual reality. However, due to significant occlusions between hands and object as well as the high degree-of-freedom motions, it is challenging to collect and annotate a high-quality, large-scale dataset, which prevents further improvement of bimanual hand-object interaction-related baselines. In this work, we propose a new 3D Gaussian Splatting based data augmentation framework for bimanual hand-object interaction, which is capable of augmenting existing dataset to large-scale photorealistic data with various hand-object pose and viewpoints. First, we use mesh-based 3DGS to model objects and hands, and to deal with the rendering blur problem due to multi-resolution input images used, we design a super-resolution module. Second, we extend the single hand grasping pose optimization module for the bimanual hand object to generate various poses of bimanual hand-object interaction, which can significantly expand the pose distribution of the dataset. Third, we conduct an analysis for the impact of different aspects of the proposed data augmentation on the understanding of the bimanual hand-object interaction. We perform our data augmentation on two benchmarks, H2O and Arctic, and verify that our method can improve the performance of the baselines.

对双手与物体交互的理解在机器人和虚拟现实领域具有重要作用。然而，由于双手和物体之间的显著遮挡以及高自由度运动的复杂性，收集和标注高质量、大规模数据集具有极大挑战性，从而限制了与双手物体交互相关的基线模型的进一步改进。在本研究中，我们提出了一种基于 3D 高斯点（3D Gaussian Splatting, 3DGS）的数据增强框架，用于双手物体交互。该框架能够将现有数据集扩展为具有多种手-物体姿态和视角的大规模真实感数据。
首先，我们使用基于网格的 3DGS 模型化物体和手，并针对多分辨率输入图像导致的渲染模糊问题设计了一个超分辨率模块。其次，我们扩展了单手抓取姿态优化模块以适应双手物体交互，从而生成多样化的双手物体交互姿态，这显著扩展了数据集的姿态分布。最后，我们对所提出数据增强方法的不同方面对双手物体交互理解的影响进行了分析。
我们在两个基准数据集 H2O 和 Arctic 上进行了数据增强实验，并验证了我们的方法可以提升基线模型的性能。


---

## [16] FatesGS: Fast and Accurate Sparse-View Surface Reconstruction using Gaussian Splatting with Depth-Feature Consistency

### FatesGS: Fast and Accurate Sparse-View Surface Reconstruction using Gaussian Splatting with Depth-Feature Consistency

Recently, Gaussian Splatting has sparked a new trend in the field of computer vision. Apart from novel view synthesis, it has also been extended to the area of multi-view reconstruction. The latest methods facilitate complete, detailed surface reconstruction while ensuring fast training speed. However, these methods still require dense input views, and their output quality significantly degrades with sparse views. We observed that the Gaussian primitives tend to overfit the few training views, leading to noisy floaters and incomplete reconstruction surfaces. In this paper, we present an innovative sparse-view reconstruction framework that leverages intra-view depth and multi-view feature consistency to achieve remarkably accurate surface reconstruction. Specifically, we utilize monocular depth ranking information to supervise the consistency of depth distribution within patches and employ a smoothness loss to enhance the continuity of the distribution. To achieve finer surface reconstruction, we optimize the absolute position of depth through multi-view projection features. Extensive experiments on DTU and BlendedMVS demonstrate that our method outperforms state-of-the-art methods with a speedup of 60x to 200x, achieving swift and fine-grained mesh reconstruction without the need for costly pre-training.

最近，高斯点绘制（Gaussian Splatting）在计算机视觉领域掀起了一股新潮流。除了用于新视图合成外，它还被扩展到多视角重建领域。最新的方法能够实现完整且细致的表面重建，同时保证较快的训练速度。然而，这些方法仍然需要密集的输入视图，当视图稀疏时，输出质量会显著下降。我们观察到，高斯原语在稀疏视图训练时容易过拟合，导致噪声浮点和不完整的重建表面。
本文提出了一种创新的稀疏视图重建框架，通过利用视内深度和多视角特征一致性，实现了高度精确的表面重建。具体而言，我们利用单目深度排序信息监督补丁内的深度分布一致性，并引入平滑损失以增强分布的连续性。为了实现更精细的表面重建，我们通过多视角投影特征优化深度的绝对位置。
在 DTU 和 BlendedMVS 数据集上的大量实验表明，所提出的方法在速度上比当前最先进的方法快 60 倍至 200 倍，同时无需昂贵的预训练即可实现快速且细粒度的网格重建。这一方法显著提升了稀疏视图重建的精度和效率。


---

## [17] BloomScene: Lightweight Structured 3D Gaussian Splatting for Crossmodal Scene Generation

### BloomScene: Lightweight Structured 3D Gaussian Splatting for Crossmodal Scene Generation

With the widespread use of virtual reality applications, 3D scene generation has become a new challenging research frontier. 3D scenes have highly complex structures and need to ensure that the output is dense, coherent, and contains all necessary structures. Many current 3D scene generation methods rely on pre-trained text-to-image diffusion models and monocular depth estimators. However, the generated scenes occupy large amounts of storage space and often lack effective regularisation methods, leading to geometric distortions. To this end, we propose BloomScene, a lightweight structured 3D Gaussian splatting for crossmodal scene generation, which creates diverse and high-quality 3D scenes from text or image inputs. Specifically, a crossmodal progressive scene generation framework is proposed to generate coherent scenes utilizing incremental point cloud reconstruction and 3D Gaussian splatting. Additionally, we propose a hierarchical depth prior-based regularization mechanism that utilizes multi-level constraints on depth accuracy and smoothness to enhance the realism and continuity of the generated scenes. Ultimately, we propose a structured context-guided compression mechanism that exploits structured hash grids to model the context of unorganized anchor attributes, which significantly eliminates structural redundancy and reduces storage overhead. Comprehensive experiments across multiple scenes demonstrate the significant potential and advantages of our framework compared with several baselines.

随着虚拟现实应用的广泛普及，3D 场景生成已成为一个具有挑战性的研究前沿。3D 场景具有高度复杂的结构，要求输出密集、连贯，并包含所有必要的结构。许多当前的 3D 场景生成方法依赖预训练的文本到图像扩散模型和单目深度估计器。然而，生成的场景通常占用大量存储空间，且缺乏有效的正则化方法，导致几何失真问题。
为此，我们提出了 BloomScene，一种基于轻量化结构化 3D 高斯点渲染的跨模态场景生成方法，可从文本或图像输入生成多样化且高质量的 3D 场景。具体而言，我们设计了一种跨模态渐进式场景生成框架，通过增量点云重建和 3D 高斯点渲染生成连贯的场景。此外，我们提出了一种基于分层深度先验的正则化机制，通过多层次的深度精度和光滑性约束，提高生成场景的真实感和连贯性。
最终，我们提出了一种结构化上下文引导的压缩机制，利用结构化哈希网格建模非组织锚点属性的上下文，有效消除结构冗余并减少存储开销。多场景的综合实验表明，与多个基线方法相比，我们的框架展现出了显著的潜力和优势。


---

## [18] Decoupling Appearance Variations with 3D Consistent Features in Gaussian Splatting

### Decoupling Appearance Variations with 3D Consistent Features in Gaussian Splatting

Gaussian Splatting has emerged as a prominent 3D representation in novel view synthesis, but it still suffers from appearance variations, which are caused by various factors, such as modern camera ISPs, different time of day, weather conditions, and local light changes. These variations can lead to floaters and color distortions in the rendered images/videos. Recent appearance modeling approaches in Gaussian Splatting are either tightly coupled with the rendering process, hindering real-time rendering, or they only account for mild global variations, performing poorly in scenes with local light changes. In this paper, we propose DAVIGS, a method that decouples appearance variations in a plug-and-play and efficient manner. By transforming the rendering results at the image level instead of the Gaussian level, our approach can model appearance variations with minimal optimization time and memory overhead. Furthermore, our method gathers appearance-related information in 3D space to transform the rendered images, thus building 3D consistency across views implicitly. We validate our method on several appearance-variant scenes, and demonstrate that it achieves state-of-the-art rendering quality with minimal training time and memory usage, without compromising rendering speeds. Additionally, it provides performance improvements for different Gaussian Splatting baselines in a plug-and-play manner.

高斯点渲染已成为新视角合成中的一种重要 3D 表示方法，但仍然面临外观变化问题。这些变化由多种因素引起，例如现代相机 ISP、不同的时间、天气条件和局部光照变化。这些因素可能导致渲染图像或视频中出现浮点伪影和颜色失真问题。目前高斯点渲染中的外观建模方法通常与渲染过程紧密耦合，限制了实时渲染的实现；或者仅能处理轻微的全局变化，在存在局部光照变化的场景中表现较差。
为此，我们提出了 DAVIGS，一种解耦外观变化的高效、模块化方法。通过在图像级别（而非高斯级别）对渲染结果进行转换，我们的方法能够以最小的优化时间和内存开销建模外观变化。此外，我们的方法在 3D 空间中收集与外观相关的信息，用于对渲染图像进行转换，从而隐式地在视角之间建立 3D 一致性。
我们在多个外观变化场景中验证了该方法，结果表明 DAVIGS 在实现最先进渲染质量的同时，显著减少了训练时间和内存使用，并且不影响渲染速度。此外，它还能以模块化的方式为不同的高斯点渲染基线方法带来性能改进。


---

## [19] Micro-macro Wavelet-based Gaussian Splatting for 3D Reconstruction from Unconstrained Images

### Micro-macro Wavelet-based Gaussian Splatting for 3D Reconstruction from Unconstrained Images

3D reconstruction from unconstrained image collections presents substantial challenges due to varying appearances and transient occlusions. In this paper, we introduce Micro-macro Wavelet-based Gaussian Splatting (MW-GS), a novel approach designed to enhance 3D reconstruction by disentangling scene representations into global, refined, and intrinsic components. The proposed method features two key innovations: Micro-macro Projection, which allows Gaussian points to capture details from feature maps across multiple scales with enhanced diversity; and Wavelet-based Sampling, which leverages frequency domain information to refine feature representations and significantly improve the modeling of scene appearances. Additionally, we incorporate a Hierarchical Residual Fusion Network to seamlessly integrate these features. Extensive experiments demonstrate that MW-GS delivers state-of-the-art rendering performance, surpassing existing methods.

从非受约束的图像集合进行3D重建面临诸多挑战，包括外观变化和瞬时遮挡等问题。本文提出了一种新颖的方法——微宏小波基高斯点云（Micro-macro Wavelet-based Gaussian Splatting, MW-GS），通过将场景表示解耦为全局、精细和内在组件，提升3D重建质量。该方法的核心创新包括：（i）微宏投影（Micro-macro Projection），使高斯点能够从不同尺度的特征图中提取信息，增强细节捕捉的多样性；（ii）基于小波的采样（Wavelet-based Sampling），利用频域信息优化特征表示，显著提升场景外观建模能力。此外，我们引入了分层残差融合网络（Hierarchical Residual Fusion Network），用于无缝集成这些特征。大量实验表明，MW-GS在渲染性能上达到了最先进水平，超越了现有方法。



---

## [20] Large Images are Gaussians: High-Quality Large Image Representation with Levels of 2D Gaussian Splatting

### Large Images are Gaussians: High-Quality Large Image Representation with Levels of 2D Gaussian Splatting

While Implicit Neural Representations (INRs) have demonstrated significant success in image representation, they are often hindered by large training memory and slow decoding speed. Recently, Gaussian Splatting (GS) has emerged as a promising solution in 3D reconstruction due to its high-quality novel view synthesis and rapid rendering capabilities, positioning it as a valuable tool for a broad spectrum of applications. In particular, a GS-based representation, 2DGS, has shown potential for image fitting. In our work, we present Large Images are Gaussians (LIG), which delves deeper into the application of 2DGS for image representations, addressing the challenge of fitting large images with 2DGS in the situation of numerous Gaussian points, through two distinct modifications: 1) we adopt a variant of representation and optimization strategy, facilitating the fitting of a large number of Gaussian points; 2) we propose a Level-of-Gaussian approach for reconstructing both coarse low-frequency initialization and fine high-frequency details. Consequently, we successfully represent large images as Gaussian points and achieve high-quality large image representation, demonstrating its efficacy across various types of large images.

尽管隐式神经表示（INRs）在图像表示方面取得了显著成功，但它们通常受到大规模训练内存和较慢解码速度的限制。最近，高斯溅射（GS）由于其高质量的新视角合成和快速渲染能力，成为3D重建领域中的一种有前景的解决方案，定位为广泛应用的有价值工具。特别是，基于GS的表示方法——2DGS，在图像拟合方面显示了潜力。在我们的研究中，我们提出了Large Images are Gaussians (LIG)，进一步探讨了2DGS在图像表示中的应用，解决了在大量高斯点情况下，如何使用2DGS拟合大图像这一挑战，具体通过两项独特的改进：1）我们采用了一种变体的表示和优化策略，便于拟合大量高斯点；2）我们提出了一种高斯层级方法，用于重建粗略的低频初始化和细致的高频细节。因此，我们成功地将大图像表示为高斯点，并实现了高质量的大图像表示，展示了其在多种类型的大图像中的有效性。


---

## [21] Efficient Gaussian Splatting for Monocular Dynamic Scene Rendering via Sparse Time-Variant Attribute Modeling

### Efficient Gaussian Splatting for Monocular Dynamic Scene Rendering via Sparse Time-Variant Attribute Modeling

Rendering dynamic scenes from monocular videos is a crucial yet challenging task. The recent deformable Gaussian Splatting has emerged as a robust solution to represent real-world dynamic scenes. However, it often leads to heavily redundant Gaussians, attempting to fit every training view at various time steps, leading to slower rendering speeds. Additionally, the attributes of Gaussians in static areas are time-invariant, making it unnecessary to model every Gaussian, which can cause jittering in static regions. In practice, the primary bottleneck in rendering speed for dynamic scenes is the number of Gaussians. In response, we introduce Efficient Dynamic Gaussian Splatting (EDGS), which represents dynamic scenes via sparse time-variant attribute modeling. Our approach formulates dynamic scenes using a sparse anchor-grid representation, with the motion flow of dense Gaussians calculated via a classical kernel representation. Furthermore, we propose an unsupervised strategy to efficiently filter out anchors corresponding to static areas. Only anchors associated with deformable objects are input into MLPs to query time-variant attributes. Experiments on two real-world datasets demonstrate that our EDGS significantly improves the rendering speed with superior rendering quality compared to previous state-of-the-art methods.

从单目视频渲染动态场景是一个关键但具有挑战性的任务。近年来，可变形高斯散点 (Deformable Gaussian Splatting) 已成为表示真实世界动态场景的强大解决方案。然而，该方法往往会引入大量冗余高斯基元，试图在不同时间步拟合所有训练视图，导致渲染速度变慢。此外，静态区域的高斯属性本质上是时间不变的，但现有方法仍对其进行建模，容易导致静态区域的抖动 (jittering)。在实际应用中，动态场景渲染速度的主要瓶颈是高斯基元的数量。
为此，我们提出高效动态高斯散点 (Efficient Dynamic Gaussian Splatting, EDGS)，通过稀疏的时间变化属性建模来表示动态场景。我们的方法采用稀疏锚点网格 (sparse anchor-grid) 表示动态场景，并利用经典核表示 (classical kernel representation) 计算稠密高斯的运动流 (motion flow)。此外，我们提出了一种无监督策略 (unsupervised strategy) 来高效过滤静态区域的锚点，仅对可变形物体关联的锚点输入 MLP 以查询时间变化属性。
在两个真实世界数据集上的实验表明，EDGS 在显著提升渲染速度的同时，渲染质量优于当前最先进方法。


---

## [22] ATLAS Navigator: Active Task-driven LAnguage-embedded Gaussian Splatting

### ATLAS Navigator: Active Task-driven LAnguage-embedded Gaussian Splatting

We address the challenge of task-oriented navigation in unstructured and unknown environments, where robots must incrementally build and reason on rich, metric-semantic maps in real time. Since tasks may require clarification or re-specification, it is necessary for the information in the map to be rich enough to enable generalization across a wide range of tasks. To effectively execute tasks specified in natural language, we propose a hierarchical representation built on language-embedded Gaussian splatting that enables both sparse semantic planning that lends itself to online operation and dense geometric representation for collision-free navigation. We validate the effectiveness of our method through real-world robot experiments conducted in both cluttered indoor and kilometer-scale outdoor environments, with a competitive ratio of about 60% against privileged baselines.

我们研究面向任务的导航 (task-oriented navigation) 在非结构化 (unstructured) 和未知环境中的挑战，其中机器人需要实时构建和推理丰富的度量-语义地图 (metric-semantic maps)。由于任务可能需要澄清或重新指定，地图中的信息必须足够丰富，以便能够泛化到各种任务。
为了使机器人能够有效执行自然语言指定的任务，我们提出了一种基于语言嵌入的高斯散点 (language-embedded Gaussian Splatting) 的分层表示。这一方法同时具备稀疏语义规划 (sparse semantic planning)，以适应在线任务执行，以及稠密几何表示 (dense geometric representation)，以确保无碰撞导航。
我们在真实世界机器人实验中验证了该方法的有效性，实验涵盖了复杂的室内环境和公里级的室外环境。在对比特权基线 (privileged baselines) 的评测中，我们的方法实现了约 60% 的竞争性比率 (competitive ratio)，展现出卓越的性能。


---

## [23] Frequency-Aware Density Control via Reparameterization for High-Quality Rendering of 3D Gaussian Splatting

### Frequency-Aware Density Control via Reparameterization for High-Quality Rendering of 3D Gaussian Splatting

By adaptively controlling the density and generating more Gaussians in regions with high-frequency information, 3D Gaussian Splatting (3DGS) can better represent scene details. From the signal processing perspective, representing details usually needs more Gaussians with relatively smaller scales. However, 3DGS currently lacks an explicit constraint linking the density and scale of 3D Gaussians across the domain, leading to 3DGS using improper-scale Gaussians to express frequency information, resulting in the loss of accuracy. In this paper, we propose to establish a direct relation between density and scale through the reparameterization of the scaling parameters and ensure the consistency between them via explicit constraints (i.e., density responds well to changes in frequency). Furthermore, we develop a frequency-aware density control strategy, consisting of densification and deletion, to improve representation quality with fewer Gaussians. A dynamic threshold encourages densification in high-frequency regions, while a scale-based filter deletes Gaussians with improper scale. Experimental results on various datasets demonstrate that our method outperforms existing state-of-the-art methods quantitatively and qualitatively.

通过自适应控制密度并在高频信息区域生成更多高斯点，三维高斯散点（3D Gaussian Splatting, 3DGS）能够更精细地表示场景细节。从信号处理的角度来看，表示高频细节通常需要更多的高斯点，且其尺度相对较小。然而，当前 3DGS 缺乏一个明确的约束来关联三维高斯点的密度与尺度，导致 3DGS 在整个空间域中采用不适当尺度的高斯点来表达频率信息，从而影响表示精度。
为了解决这一问题，本文提出通过缩放参数的重参数化（reparameterization of the scaling parameters）建立密度与尺度的直接关系，并引入显式约束以确保它们之间的一致性（即密度能够准确响应频率变化）。此外，我们设计了一种频率感知的密度控制策略（frequency-aware density control strategy），包括加密（densification）和删除（deletion）机制，以在减少高斯点数量的同时提升表示质量。具体而言，我们引入动态阈值以在高频区域促进加密，同时采用**基于尺度的滤波（scale-based filter）**来删除尺度不当的高斯点。
在多个数据集上的实验结果表明，我们的方法在定量和定性评测上均超越了现有的最先进方法（state-of-the-art, SOTA），有效提升了 3DGS 的表示精度和渲染质量。


---

## [24] Enhancing Close-up Novel View Synthesis via Pseudo-labeling

### Enhancing Close-up Novel View Synthesis via Pseudo-labeling

Recent methods, such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), have demonstrated remarkable capabilities in novel view synthesis. However, despite their success in producing high-quality images for viewpoints similar to those seen during training, they struggle when generating detailed images from viewpoints that significantly deviate from the training set, particularly in close-up views. The primary challenge stems from the lack of specific training data for close-up views, leading to the inability of current methods to render these views accurately. To address this issue, we introduce a novel pseudo-label-based learning strategy. This approach leverages pseudo-labels derived from existing training data to provide targeted supervision across a wide range of close-up viewpoints. Recognizing the absence of benchmarks for this specific challenge, we also present a new dataset designed to assess the effectiveness of both current and future methods in this area. Our extensive experiments demonstrate the efficacy of our approach.

近年来，神经辐射场 (NeRF) 和 3D 高斯散点 (3DGS) 等方法在新视角合成方面展现出了卓越的能力。然而，尽管它们在生成与训练视角相似的高质量图像方面取得了成功，但在生成与训练集存在较大偏差的视角，特别是近景视角时，仍然存在明显的挑战。这一问题的核心原因在于缺乏针对近景视角的特定训练数据，导致现有方法难以精确渲染这些视角的细节。
为了解决这一问题，我们提出了一种基于伪标签的学习策略。该方法利用从已有训练数据中生成的伪标签，为广泛的近景视角提供有针对性的监督，从而提升渲染质量。此外，考虑到当前缺乏专门针对该挑战的基准测试，我们构建了一个新数据集，用于评估现有方法和未来方法在这一问题上的表现。
大量实验结果表明，我们的方法能够有效提升近景视角的渲染质量，并在该任务上取得了显著的性能提升。


---

